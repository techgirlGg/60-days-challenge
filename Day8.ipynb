{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ce87667-d19a-4553-a290-d2d8653d94a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tokenizers in c:\\users\\krish\\appdata\\roaming\\python\\python312\\site-packages (0.22.2)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in c:\\users\\krish\\appdata\\roaming\\python\\python312\\site-packages (from tokenizers) (1.4.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (2024.6.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\krish\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (0.27.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (6.0.1)\n",
      "Requirement already satisfied: shellingham in c:\\users\\krish\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.66.5)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\krish\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.11.0)\n",
      "Requirement already satisfied: anyio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<2.0,>=0.16.4->tokenizers) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer-slim->huggingface-hub<2.0,>=0.16.4->tokenizers) (8.1.7)\n",
      "âœ… Corpus Ready\n",
      "âœ… Tokenizer Training Completed\n",
      "âœ… Tokenizer Saved\n",
      "\n",
      "ðŸ“Š Vocabulary Size: 198\n",
      "\n",
      "ðŸ”¹ Sample Vocabulary (Top 20 Tokens):\n",
      "Mach : 169\n",
      "##ep : 138\n",
      "models : 114\n",
      "Natural : 190\n",
      "##iec : 100\n",
      "A : 5\n",
      "##r : 47\n",
      "##okeniz : 90\n",
      "Tokenization : 171\n",
      "of : 126\n",
      "R : 13\n",
      "subwords : 196\n",
      "##ts : 148\n",
      "Tra : 120\n",
      "##ng : 71\n",
      "##on : 78\n",
      "##ni : 70\n",
      "de : 124\n",
      "subword : 181\n",
      "learning : 113\n",
      "\n",
      "ðŸ§ª Sample Text: WordPiece tokenizer is powerful\n",
      "Tokens: ['WordPiece', 'tokenizer', 'is', 'powerful']\n",
      "Token IDs: [116, 166, 74, 194]\n",
      "\n",
      "ðŸ“Š Vocabulary Size Comparison\n",
      "Requested: 100 | Actual Learned: 100\n",
      "Requested: 300 | Actual Learned: 198\n",
      "Requested: 500 | Actual Learned: 196\n",
      "Requested: 1000 | Actual Learned: 198\n",
      "\n",
      "âœ… All Steps Completed Successfully\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# WordPiece Tokenizer Training + Analysis\n",
    "# ============================================\n",
    "\n",
    "# ---------------------------\n",
    "# Install (Run once if needed)\n",
    "# ---------------------------\n",
    "!pip install tokenizers\n",
    "# ---------------------------\n",
    "# Import Libraries\n",
    "# ---------------------------\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import os\n",
    "\n",
    "# ---------------------------\n",
    "# Step 1: Create Corpus File (If Not Exists)\n",
    "# ---------------------------\n",
    "corpus_file = \"corpus.txt\"\n",
    "\n",
    "if not os.path.exists(corpus_file):\n",
    "    corpus = [\n",
    "        \"Natural language processing is amazing\",\n",
    "        \"WordPiece tokenizer is used in BERT models\",\n",
    "        \"Tokenization splits text into subwords\",\n",
    "        \"NLP is widely used in AI applications\",\n",
    "        \"Machine learning and deep learning are part of AI\",\n",
    "        \"Transformers are powerful NLP models\",\n",
    "        \"BERT uses WordPiece tokenization technique\"\n",
    "    ]\n",
    "\n",
    "    with open(corpus_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in corpus:\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "print(\"âœ… Corpus Ready\")\n",
    "\n",
    "# ---------------------------\n",
    "# Step 2: Initialize Tokenizer\n",
    "# ---------------------------\n",
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# ---------------------------\n",
    "# Step 3: Train Tokenizer\n",
    "# ---------------------------\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=1000,\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    ")\n",
    "\n",
    "tokenizer.train([corpus_file], trainer)\n",
    "\n",
    "print(\"âœ… Tokenizer Training Completed\")\n",
    "\n",
    "# ---------------------------\n",
    "# Step 4: Save Tokenizer\n",
    "# ---------------------------\n",
    "tokenizer.save(\"wordpiece_tokenizer.json\")\n",
    "print(\"âœ… Tokenizer Saved\")\n",
    "\n",
    "# ---------------------------\n",
    "# Step 5: Vocabulary Analysis\n",
    "# ---------------------------\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "print(\"\\nðŸ“Š Vocabulary Size:\", len(vocab))\n",
    "\n",
    "print(\"\\nðŸ”¹ Sample Vocabulary (Top 20 Tokens):\")\n",
    "vocab_items = list(vocab.items())[:20]\n",
    "\n",
    "for token, idx in vocab_items:\n",
    "    print(token, \":\", idx)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 6: Test Tokenization\n",
    "# ---------------------------\n",
    "sample_text = \"WordPiece tokenizer is powerful\"\n",
    "\n",
    "encoded = tokenizer.encode(sample_text)\n",
    "\n",
    "print(\"\\nðŸ§ª Sample Text:\", sample_text)\n",
    "print(\"Tokens:\", encoded.tokens)\n",
    "print(\"Token IDs:\", encoded.ids)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 7: Compare Different Vocabulary Sizes\n",
    "# ---------------------------\n",
    "print(\"\\nðŸ“Š Vocabulary Size Comparison\")\n",
    "\n",
    "for size in [100, 300, 500, 1000]:\n",
    "    temp_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "    temp_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    temp_trainer = WordPieceTrainer(\n",
    "        vocab_size=size,\n",
    "        special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "    )\n",
    "\n",
    "    temp_tokenizer.train([corpus_file], temp_trainer)\n",
    "\n",
    "    temp_vocab = temp_tokenizer.get_vocab()\n",
    "\n",
    "    print(f\"Requested: {size} | Actual Learned: {len(temp_vocab)}\")\n",
    "\n",
    "print(\"\\nâœ… All Steps Completed Successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5556e0c-b4b7-4108-9578-90adabcc2be3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
